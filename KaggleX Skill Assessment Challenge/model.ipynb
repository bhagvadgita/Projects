{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id    brand          model  model_year  milage fuel_type   \n",
      "0   0     Ford   F-150 Lariat        2018   74349  Gasoline  \\\n",
      "1   1      BMW          335 i        2007   80000  Gasoline   \n",
      "2   2   Jaguar      XF Luxury        2009   91491  Gasoline   \n",
      "3   3      BMW   X7 xDrive40i        2022    2437    Hybrid   \n",
      "4   4  Pontiac  Firebird Base        2001  111000  Gasoline   \n",
      "\n",
      "                                              engine   \n",
      "0      375.0HP 3.5L V6 Cylinder Engine Gasoline Fuel  \\\n",
      "1  300.0HP 3.0L Straight 6 Cylinder Engine Gasoli...   \n",
      "2       300.0HP 4.2L 8 Cylinder Engine Gasoline Fuel   \n",
      "3  335.0HP 3.0L Straight 6 Cylinder Engine Gasoli...   \n",
      "4      200.0HP 3.8L V6 Cylinder Engine Gasoline Fuel   \n",
      "\n",
      "                     transmission ext_col int_col       accident clean_title   \n",
      "0                    10-Speed A/T    Blue    Gray  None reported         Yes  \\\n",
      "1                     6-Speed M/T   Black   Black  None reported         Yes   \n",
      "2                     6-Speed A/T  Purple   Beige  None reported         Yes   \n",
      "3  Transmission w/Dual Shift Mode    Gray   Brown  None reported         Yes   \n",
      "4                             A/T   White   Black  None reported         Yes   \n",
      "\n",
      "   price  \n",
      "0  11000  \n",
      "1   8250  \n",
      "2  15000  \n",
      "3  63500  \n",
      "4   7850  \n",
      "      id          brand                                              model   \n",
      "0  54273  Mercedes-Benz                                      E-Class E 350  \\\n",
      "1  54274          Lexus                                        RX 350 Base   \n",
      "2  54275  Mercedes-Benz                                      C-Class C 300   \n",
      "3  54276           Land  Rover Range Rover 5.0L Supercharged Autobiogra...   \n",
      "4  54277            BMW                                       X6 xDrive40i   \n",
      "\n",
      "   model_year  milage fuel_type   \n",
      "0        2014   73000  Gasoline  \\\n",
      "1        2015  128032  Gasoline   \n",
      "2        2015   51983  Gasoline   \n",
      "3        2018   29500  Gasoline   \n",
      "4        2020   90000  Gasoline   \n",
      "\n",
      "                                              engine   \n",
      "0      302.0HP 3.5L V6 Cylinder Engine Gasoline Fuel  \\\n",
      "1      275.0HP 3.5L V6 Cylinder Engine Gasoline Fuel   \n",
      "2       241.0HP 2.0L 4 Cylinder Engine Gasoline Fuel   \n",
      "3       518.0HP 5.0L 8 Cylinder Engine Gasoline Fuel   \n",
      "4  335.0HP 3.0L Straight 6 Cylinder Engine Gasoli...   \n",
      "\n",
      "                     transmission ext_col int_col   \n",
      "0                             A/T   White   Beige  \\\n",
      "1                     8-Speed A/T  Silver   Black   \n",
      "2                     7-Speed A/T    Blue   White   \n",
      "3  Transmission w/Dual Shift Mode   White   White   \n",
      "4                     8-Speed A/T   White   Black   \n",
      "\n",
      "                                 accident clean_title  \n",
      "0                           None reported         Yes  \n",
      "1                           None reported         Yes  \n",
      "2                           None reported         Yes  \n",
      "3  At least 1 accident or damage reported         Yes  \n",
      "4  At least 1 accident or damage reported         Yes  \n",
      "                 id    model_year         milage         price\n",
      "count  54273.000000  54273.000000   54273.000000  5.427300e+04\n",
      "mean   27136.000000   2015.091979   72746.175667  3.921844e+04\n",
      "std    15667.409917      5.588909   50469.490448  7.282634e+04\n",
      "min        0.000000   1974.000000     100.000000  2.000000e+03\n",
      "25%    13568.000000   2012.000000   32268.000000  1.550000e+04\n",
      "50%    27136.000000   2016.000000   66107.000000  2.800000e+04\n",
      "75%    40704.000000   2019.000000  102000.000000  4.500000e+04\n",
      "max    54272.000000   2024.000000  405000.000000  2.954083e+06\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54273 entries, 0 to 54272\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            54273 non-null  int64 \n",
      " 1   brand         54273 non-null  object\n",
      " 2   model         54273 non-null  object\n",
      " 3   model_year    54273 non-null  int64 \n",
      " 4   milage        54273 non-null  int64 \n",
      " 5   fuel_type     54273 non-null  object\n",
      " 6   engine        54273 non-null  object\n",
      " 7   transmission  54273 non-null  object\n",
      " 8   ext_col       54273 non-null  object\n",
      " 9   int_col       54273 non-null  object\n",
      " 10  accident      54273 non-null  object\n",
      " 11  clean_title   54273 non-null  object\n",
      " 12  price         54273 non-null  int64 \n",
      "dtypes: int64(4), object(9)\n",
      "memory usage: 5.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('train.csv/train.csv')\n",
    "test_data = pd.read_csv('test.csv/test.csv')\n",
    "\n",
    "# Display the first few rows of the training data\n",
    "print(train_data.head())\n",
    "# Display the first few rows of the test data\n",
    "print(test_data.head())\n",
    "\n",
    "# Display summary statistics of the training data\n",
    "print(train_data.describe())\n",
    "# Display information about the training data\n",
    "print(train_data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n",
      "Best parameters found:  {'model__colsample_bytree': 0.6, 'model__learning_rate': 0.01, 'model__max_depth': 7, 'model__n_estimators': 200, 'model__subsample': 0.6}\n",
      "Root Mean Squared Error with best model: 61735.105681607434\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "X = train_data.drop(['id', 'price'], axis=1)\n",
    "y = train_data['price']\n",
    "\n",
    "# Select categorical and numerical columns\n",
    "categorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]\n",
    "numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define the model\n",
    "model = xgb.XGBRegressor(random_state=0)\n",
    "\n",
    "# Create a pipeline that preprocesses the data and then fits the model\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)])\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__subsample': [0.6, 0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, \n",
    "                           cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Split data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Train the final model with best parameters\n",
    "# Note: We use ColumnTransformer to preprocess the data before fitting the model\n",
    "best_model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('model', xgb.XGBRegressor(**{k.split('__')[1]: v for k, v in best_params.items()}))])\n",
    "\n",
    "best_model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "preds = best_model_pipeline.predict(X_valid)\n",
    "rmse = np.sqrt(mean_squared_error(y_valid, preds))\n",
    "print('Root Mean Squared Error with best model:', rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "test_preds = best_model_pipeline.predict(test_data.drop(['id'], axis=1))\n",
    "\n",
    "# Create a DataFrame with the ID and predicted price\n",
    "submission = pd.DataFrame({'id': test_data['id'], 'price': test_preds})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 128 candidates, totalling 384 fits\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "X = train_data.drop(['id', 'price'], axis=1)\n",
    "y = train_data['price']\n",
    "\n",
    "# Select categorical and numerical columns\n",
    "categorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]\n",
    "numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define the base models\n",
    "base_models = [\n",
    "    ('xgb', xgb.XGBRegressor(random_state=0)),\n",
    "    ('rf', RandomForestRegressor(random_state=0))\n",
    "]\n",
    "\n",
    "# Define the meta-model\n",
    "meta_model = RidgeCV()\n",
    "\n",
    "# Create the stacking model\n",
    "stacking_model = StackingRegressor(estimators=base_models, final_estimator=meta_model, n_jobs=-1)\n",
    "\n",
    "# Bundle preprocessing and stacking model in a pipeline\n",
    "stacking_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                    ('stacking', stacking_model)])\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'stacking__xgb__n_estimators': [100, 200],\n",
    "    'stacking__xgb__learning_rate': [0.01, 0.05],\n",
    "    'stacking__xgb__max_depth': [3, 5],\n",
    "    'stacking__xgb__subsample': [0.6, 0.8],\n",
    "    'stacking__xgb__colsample_bytree': [0.6, 0.8],\n",
    "    'stacking__rf__n_estimators': [100, 200],\n",
    "    'stacking__rf__max_depth': [None, 10]\n",
    "}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(estimator=stacking_pipeline, param_grid=param_grid, \n",
    "                           cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Split data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Train the final stacking model with best parameters\n",
    "best_stacking_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model\n",
    "preds = best_stacking_model.predict(X_valid)\n",
    "rmse = np.sqrt(mean_squared_error(y_valid, preds))\n",
    "print('Root Mean Squared Error with best stacking model:', rmse)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_preds = best_stacking_model.predict(test_data.drop(['id'], axis=1))\n",
    "\n",
    "# Create a DataFrame with the ID and predicted price\n",
    "submission = pd.DataFrame({'id': test_data['id'], 'price': test_preds})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission.to_csv('xgboost_with_stacking.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stacking_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m\n\u001b[0;32m      4\u001b[0m param_dist \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstacking__xgb__n_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstacking__xgb__learning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.05\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstacking__rf__max_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Set up the randomized search\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(estimator\u001b[38;5;241m=\u001b[39m\u001b[43mstacking_pipeline\u001b[49m, param_distributions\u001b[38;5;241m=\u001b[39mparam_dist, \n\u001b[0;32m     16\u001b[0m                                    n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Fit the randomized search\u001b[39;00m\n\u001b[0;32m     19\u001b[0m random_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stacking_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the parameter distributions for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'stacking__xgb__n_estimators': [100, 200],\n",
    "    'stacking__xgb__learning_rate': [0.01, 0.05],\n",
    "    'stacking__xgb__max_depth': [3, 5],\n",
    "    'stacking__xgb__subsample': [0.6, 0.8],\n",
    "    'stacking__xgb__colsample_bytree': [0.6, 0.8],\n",
    "    'stacking__rf__n_estimators': [100, 200],\n",
    "    'stacking__rf__max_depth': [None, 10]\n",
    "}\n",
    "\n",
    "# Set up the randomized search\n",
    "random_search = RandomizedSearchCV(estimator=stacking_pipeline, param_distributions=param_dist, \n",
    "                                   n_iter=50, cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error', random_state=0)\n",
    "\n",
    "# Fit the randomized search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Train the final stacking model with best parameters\n",
    "best_stacking_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the model\n",
    "preds = best_stacking_model.predict(X_valid)\n",
    "rmse = np.sqrt(mean_squared_error(y_valid, preds))\n",
    "print('Root Mean Squared Error with best stacking model:', rmse)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_preds = best_stacking_model.predict(test_data.drop(['id'], axis=1))\n",
    "\n",
    "# Create a DataFrame with the ID and predicted price\n",
    "submission = pd.DataFrame({'id': test_data['id'], 'price': test_preds})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission.to_csv('Randomized_search_cv.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
